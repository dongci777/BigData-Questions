# 大数据开发问题总结

## 一、Flink作业

### 1、前言

Flink作业数据流向：

1. 数据库到数据库
2. Kafka到数据库
3. 转储

### 2、问题重现和分析

作业一：xxx指标分析 

我们的数据源获取通过连接postgresql数据库进行获取，该数据源是其他的Flink作业结合生成的，我们只取Metrics为我们自己想要的那些数据

```scala
val url = s"jdbc:postgresql://${configUtil.rdsIP}:${configUtil.rdsPort}/${configUtil.rdsDatabase}"; 
val tableName = 'dwd.表名' 
val metric = 'dwd.表名.metric' 
```

根据上面的信息可以获取到对应的数据源，然后经过一系列的处理，将数据存到关系型数据库中dwd.表名，和存储到文件系统中（hdfs)，比如/user/项目名/metrics目录下

作业运行问题：

执行作业脚本之后，作业正常运行，生成了文件系统目录，但是没有对应的文件输出

解决方案：（Kafka模拟，编写一个Job通过Kafka来提供数据）

仔细检查代码分析，原来文件的输出依赖于当前时间戳（而我们的数据是很久之前旧数据通过回放得到的），通过Kafka灌流方式模拟数据最终解决问题。





## 二、Spark作业

### 1、问题重新和分析

作业一：初始化创建数据库和数据外表

主要是创建数据库，创建JDBC外表，创建HDFS外表

问题：spark作业提交后，无法自己finish，导致其他作业一直无法提交成功

解决：在执行完任务之后执行spark.close()来进行手动释放



作业二：插入数据作业

问题：通过两个表连接之后插入数据到第三个表中出现找不到字段问题

原因：创建外表的时候依赖的文件没有该字段，但是却去查询该字段，导致问题发生。



